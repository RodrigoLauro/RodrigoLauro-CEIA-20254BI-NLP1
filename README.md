ğŸ“‘ Contenido del Repositorio

ğŸ§© DesafÃ­o 1 â€“ VectorizaciÃ³n de Texto & NaÃ¯ve Bayes

â€œDel texto al nÃºmero: el primer puente del NLP.â€

En este desafÃ­o trabajÃ© con el dataset 20 Newsgroups, aplicando tÃ©cnicas clÃ¡sicas de representaciÃ³n:

  - TF-IDF, CountVectorizer

  - Similaridad de coseno entre documentos

  - Un clasificador Naive Bayes optimizado

  - Un sistema tipo zero-shot usando comparaciÃ³n por prototipos

  - AnÃ¡lisis de similitud entre palabras a partir de la matriz transpuesta tÃ©rmino-documento

  Este trabajo introduce cÃ³mo transformar texto bruto en vectores Ãºtiles para clasificaciÃ³n.

ğŸ§© DesafÃ­o 2 â€“ Word Embeddings con Word2Vec

â€œLas palabras adquieren significado en el espacio vectorial.â€

EntrenÃ© embeddings Word2Vec (Skip-Gram) usando un corpus propio de letras de canciones.
Incluye:

  - Preprocesamiento y tokenizaciÃ³n

  - Entrenamiento con negative sampling

  - VisualizaciÃ³n en 3D con t-SNE

  -AnÃ¡lisis de similitud semÃ¡ntica entre palabras

El objetivo fue comprender cÃ³mo los modelos modernos capturan relaciones profundas entre palabras.

ğŸ§© DesafÃ­o 3 â€“ Modelo de Lenguaje a Nivel CarÃ¡cter

â€œPredecir el siguiente carÃ¡cter: la esencia del lenguaje.â€

EntrenÃ© un modelo SimpleRNN capaz de predecir el prÃ³ximo carÃ¡cter en un corpus completo de El PrÃ­ncipe (Maquiavelo):

  - TokenizaciÃ³n a nivel carÃ¡cter

  - Ventanas deslizantes (many-to-many)

  - MÃ©trica de perplejidad

GeneraciÃ³n de texto con:

  - Greedy Search

  - Beam Search determinÃ­stico y estocÃ¡stico

Este modelo permite entender cÃ³mo las redes recurrentes aprenden dependencias a lo largo de una secuencia.

ğŸ§© DesafÃ­o 4 â€“ Modelo Seq2Seq con LSTM para TraducciÃ³n

â€œConstruyendo un traductor paso a paso, sin usar atenciÃ³n.â€

El desafÃ­o final consiste en implementar un modelo encoderâ€“decoder con LSTM para traducir inglÃ©s â†’ espaÃ±ol usando el dataset spa-eng.

Incluye:

  - Limpieza y tokenizaciÃ³n del corpus

  - Embeddings preentrenados GloVe en el encoder

  - Decoder con LSTM entrenable

  - Teacher Forcing

Beam Search para mejorar la calidad de las traducciones

Ajustes progresivos:

  - ampliaciÃ³n del dataset

  - cambios en LR del optimizador

  - tuning de hiperparÃ¡metros

  - mejoras en estabilidad del pipeline

sin utilizar el mecanismo de atenciÃ³n, porque no forma parte del desafÃ­o

El resultado es un traductor funcional y una exploraciÃ³n completa del enfoque seq2seq clÃ¡sico que precediÃ³ a los Transformers.

ğŸŒŸ Resumen del Aprendizaje

Este repositorio refleja un recorrido ordenado por las principales etapas del NLP moderno:

  * VectorizaciÃ³n clÃ¡sica (BOW, TF-IDF)

  * Embeddings densos (Word2Vec)

  * Modelos recurrentes (RNN / LSTM)

  * Modelos seq2seq y generaciÃ³n de lenguaje

  * Cada proyecto construye sobre el anterior, culminando en un sistema completo de traducciÃ³n automÃ¡tica basado en LSTM.

ğŸ“¬ Contacto

Rodrigo Lauro
ing.rodrigo.lauro@gmail.com
NÂº SIU: a2120
