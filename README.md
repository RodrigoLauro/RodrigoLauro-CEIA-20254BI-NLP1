üìë Contenido del Repositorio

‚ñö Desaf√≠o 1 ‚Äì Vectorizaci√≥n de Texto & Na√Øve Bayes

‚ÄúDel texto al n√∫mero: el primer puente del NLP.‚Äù

En este desaf√≠o trabaj√© con el dataset 20 Newsgroups, aplicando t√©cnicas cl√°sicas de representaci√≥n:

  - TF-IDF, CountVectorizer

  - Similaridad de coseno entre documentos

  - Un clasificador Naive Bayes optimizado

  - Un sistema tipo zero-shot usando comparaci√≥n por prototipos

  - An√°lisis de similitud entre palabras a partir de la matriz transpuesta t√©rmino-documento

  Este trabajo introduce c√≥mo transformar texto bruto en vectores √∫tiles para clasificaci√≥n.

‚ñö Desaf√≠o 2 ‚Äì Word Embeddings con Word2Vec

‚ÄúLas palabras adquieren significado en el espacio vectorial.‚Äù

Entren√© embeddings Word2Vec (Skip-Gram) usando un corpus propio de letras de canciones.
Incluye:

  - Preprocesamiento y tokenizaci√≥n

  - Entrenamiento con negative sampling

  - Visualizaci√≥n en 3D con t-SNE

  -An√°lisis de similitud sem√°ntica entre palabras

El objetivo fue comprender c√≥mo los modelos modernos capturan relaciones profundas entre palabras.

‚ñö Desaf√≠o 3 ‚Äì Modelo de Lenguaje a Nivel Car√°cter

‚ÄúPredecir el siguiente car√°cter: la esencia del lenguaje.‚Äù

Entren√© un modelo SimpleRNN capaz de predecir el pr√≥ximo car√°cter en un corpus completo de El Pr√≠ncipe (Maquiavelo):

  - Tokenizaci√≥n a nivel car√°cter

  - Ventanas deslizantes (many-to-many)

  - M√©trica de perplejidad

Generaci√≥n de texto con:

  - Greedy Search

  - Beam Search determin√≠stico y estoc√°stico

Este modelo permite entender c√≥mo las redes recurrentes aprenden dependencias a lo largo de una secuencia.

‚ñö Desaf√≠o 4 ‚Äì Modelo Seq2Seq con LSTM para Traducci√≥n

‚ÄúConstruyendo un traductor paso a paso, sin usar atenci√≥n.‚Äù

El desaf√≠o final consiste en implementar un modelo encoder‚Äìdecoder con LSTM para traducir ingl√©s ‚Üí espa√±ol usando el dataset spa-eng.

Incluye:

  - Limpieza y tokenizaci√≥n del corpus

  - Embeddings preentrenados GloVe en el encoder

  - Decoder con LSTM entrenable

  - Teacher Forcing

Beam Search para mejorar la calidad de las traducciones

Ajustes progresivos:

  - ampliaci√≥n del dataset

  - cambios en LR del optimizador

  - tuning de hiperpar√°metros

  - mejoras en estabilidad del pipeline

sin utilizar el mecanismo de atenci√≥n, porque no forma parte del desaf√≠o

El resultado es un traductor funcional y una exploraci√≥n completa del enfoque seq2seq cl√°sico que precedi√≥ a los Transformers.

üìò Resumen del Aprendizaje

Este repositorio refleja un recorrido ordenado por las principales etapas del NLP moderno:

  * Vectorizaci√≥n cl√°sica (BOW, TF-IDF)

  * Embeddings densos (Word2Vec)

  * Modelos recurrentes (RNN / LSTM)

  * Modelos seq2seq y generaci√≥n de lenguaje

  * Cada proyecto construye sobre el anterior, culminando en un sistema completo de traducci√≥n autom√°tica basado en LSTM.

üì¨ Contacto

Rodrigo Lauro
ing.rodrigo.lauro@gmail.com
N¬∫ SIU: a2120
