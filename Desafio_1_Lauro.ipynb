{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Desafio 1\n",
        "  - Lauro, Rodrigo (ing.rodrigo.lauro@gmail.com)\n",
        "  - Nº SIU a2120"
      ],
      "metadata": {
        "id": "dXbYoN1mPS6I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLyYEVvfyw9q",
        "outputId": "eecfce06-a127-478c-9218-ba27adcba034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftPlyanuak8n",
        "outputId": "996b53bc-7493-45ed-9736-38958fc6b494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To what follows, our moderator has already answered the charge of \n",
            "arrogance more ably that I could have done so, so I will confine\n",
            "myself to answering the charge of illogic.\n",
            " \n",
            "\n",
            "\n",
            "This is how everyone in the western intellectual tradition is, or was,\n",
            "taught to think. It is the fundamental premis \"A is not not-A\". If a thing\n",
            "is true then its converse is necessarilly false. Without this basic \n",
            "asumption theology and science as we know them are alike impossible. We\n",
            "should distinguish the strong and weak meanings of the word \"believe\",\n",
            "however. The weak sense means I am not sure. \"I believe Tom went to \n",
            "the library.\" (but he could have gone to the track). The strong sense\n",
            "means I am so certain that I use it as a basis of thought. \"I believe \n",
            "that nature operates according to certain fundamental laws.\" (despite \n",
            "the fact that nature *appears* capricious and unpredictable). Christian\n",
            "belief is of the strong kind. (Though Christians may well hold beliefs\n",
            "of the weak kind on any number of theological and ecclesiological \n",
            "topics.)\n",
            " \n",
            "\n",
            "Note that these are two separate ideas. Most hold the first view, but the \n",
            "majority do not hold the second. Is is again a matter of pure logic that\n",
            "if Christanity is true, then Hinduism (for example) must necessarilly be\n",
            "false, insofar as it contradicts or is incompatible with, Christaianity. \n",
            "(And, as a matter of *logic*, vice versa.)\n",
            " \n",
            "\n",
            "It is arrogant to claim to know what *anyone* thinks or wants, unless \n",
            "they have told you. Christians believe God has told us what he thinks\n",
            "and wants.\n",
            "\n",
            "\n",
            "Most Christians do not base their belief on the Bible, but on the living\n",
            "tradition of the Church established by Christ and guided constantly\n",
            "by the Holy Spirit. The Bible is simply the written core of that tradition.\n",
            "\n",
            "\n",
            "If depends what you mean by differing. If I believe Tom is six feet\n",
            "tall and you believe he weighs 200 pounds, our beliefs differ, but we \n",
            "may both be right. If I believe Tom is six feet tall and you beleive\n",
            "that he is four foot nine, one of us, at least, must be wrong.\n",
            " \n",
            "\n",
            "Thus you believe that there is a single truth but that no human being \n",
            "can find it. You assert that anyone who believe that we can find \n",
            "absolute truth is mistaken. In short, you believe that anyone who\n",
            "does not share your belief on this point is wrong. QED.\n",
            "\n",
            "\n",
            "Here I begin to suspect that your real difficulty is not with the\n",
            "knowability of truth, but simply with language. Saying that the glass \n",
            "is half empty is not a contradiction of the statement that it is half\n",
            "full: it is the same fact expressed in different words. (The whole\n",
            "point of this phrase is to illustrate the different ways the pessimist\n",
            "and the optimist express the *same* fact.)\n",
            " \n",
            "It is, of course, quite true that different people may express the \n",
            "same belief in different words. It is also true that they may fail\n",
            "to understand each other's words as expressions of the same belief\n",
            "and may argue bitterly and believe that they are miles apart. Great\n",
            "scisms have occurred in just this way, and much ecumenical work has\n",
            "been done simply in resolving differences in language which conceal\n",
            "agreement in belief. This does not mean, in any sense, that all beliefs\n",
            "are equally valid. Since some of the beliefs people hold contradict\n",
            "some other beliefs that other people hold, after all obfuscations\n",
            "of language and culture in the expression of those beliefs have\n",
            "been stripped away, some of the beliefs that some people hold must,\n",
            "**necessarilly** be false, and it is neither arrogant nor illogical\n",
            "to say so. If I believe X and you believe Y we may both be correct, \n",
            "but if Y is equivalent to not-X then one of us is wrong and as long\n",
            "as we hold our respective beliefs, we must each regard the other \n",
            "as in error.\n"
          ]
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "print(newsgroups_train.data[210])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "875995a9-c176-4ff3-c62a-5c172b9ad166"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "Cantidad de documentos: 11314\n",
            "Tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'Cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'Tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "3b4f3a77-7190-443b-cdda-fe7f9cd54f51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# una vez fiteado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de términos a índices.\n",
        "# El índice es la posición en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "outputs": [],
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa-AgWrMSHM",
        "outputId": "2cfd7b38-d0c3-48cb-ba9c-c18ad63307b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "5013834c-e609-4de3-968e-f7c5b339918d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXCICFSd_y90"
      },
      "source": [
        "## Similaridad de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "8d9a9654-fbb9-46b1-8db5-dc925239b9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE WHITE HOUSE\n",
            "\n",
            "                  Office of the Press Secretary\n",
            "                   (Pittsburgh, Pennslyvania)\n",
            "______________________________________________________________\n",
            "For Immediate Release                         April 17, 1993     \n",
            "\n",
            "             \n",
            "                  RADIO ADDRESS TO THE NATION \n",
            "                        BY THE PRESIDENT\n",
            "             \n",
            "                Pittsburgh International Airport\n",
            "                    Pittsburgh, Pennsylvania\n",
            "             \n",
            "             \n",
            "10:06 A.M. EDT\n",
            "             \n",
            "             \n",
            "             THE PRESIDENT:  Good morning.  My voice is coming to\n",
            "you this morning through the facilities of the oldest radio\n",
            "station in America, KDKA in Pittsburgh.  I'm visiting the city to\n",
            "meet personally with citizens here to discuss my plans for jobs,\n",
            "health care and the economy.  But I wanted first to do my weekly\n",
            "broadcast with the American people. \n",
            "             \n",
            "             I'm told this station first broadcast in 1920 when\n",
            "it reported that year's presidential elections.  Over the past\n",
            "seven decades presidents have found ways to keep in touch with\n",
            "the people, from whistle-stop tours to fire-side chats to the bus\n",
            "tour that I adopted, along with Vice President Gore, in last\n",
            "year's campaign.\n",
            "             \n",
            "             Every Saturday morning I take this time to talk with\n",
            "you, my fellow Americans, about the problems on your minds and\n",
            "what I'm doing to try and solve them.  It's my way of reporting\n",
            "to you and of giving you a way to hold me accountable.\n",
            "             \n",
            "             You sent me to Washington to get our government and\n",
            "economy moving after years of paralysis and policy and a bad\n",
            "experiment with trickle-down economics.  You know how important\n",
            "it is for us to make bold, comprehensive changes in the way we do\n",
            "business.  \n",
            "             \n",
            "             We live in a competitive global economy.  Nations\n",
            "rise and fall on the skills of their workers, the competitiveness\n",
            "of their companies, the imagination of their industries, and the\n",
            "cooperative experience and spirit that exists between business,\n",
            "labor and government.  Although many of the economies of the\n",
            "industrialized world are now suffering from slow growth, they've\n",
            "made many of the smart investments and the tough choices which\n",
            "our government has for too long ignored.  That's why many of them\n",
            "have been moving ahead and too many of our people have been\n",
            "falling behind.\n",
            "             \n",
            "             We have an economy today that even when it grows is\n",
            "not producing new jobs.  We've increased the debt of our nation\n",
            "by four times over the last 12 years, and we don't have much to\n",
            "show for it.  We know that wages of most working people have\n",
            "stopped rising, that most people are working longer work weeks\n",
            "and that too many families can no longer afford the escalating\n",
            "cost of health care.\n",
            "             \n",
            "             But we also know that, given the right tools, the\n",
            "right incentives and the right encouragement, our workers and\n",
            "businesses can make the kinds of products and profits our economy\n",
            "needs to expand opportunity and to make our communities better\n",
            "places to live.\n",
            "             \n",
            "             In many critical products today Americans are the\n",
            "low cost, high quality producers.  Our task is to make sure that\n",
            "we create more of those kinds of jobs.\n",
            "             \n",
            "             Just two months ago I gave Congress my plan for\n",
            "long-term jobs and economic growth.  It changes the old\n",
            "priorities in Washington and puts our emphasis where it needs to\n",
            "be -- on people's real needs, on increasing investments and jobs\n",
            "and education, on cutting the federal deficit, on stopping the\n",
            "waste which pays no dividends, and redirecting our precious\n",
            "resources toward investment that creates jobs now and lays the\n",
            "groundwork for robust economic growth in the future.\n",
            "             \n",
            "             These new directions passed the Congress in record\n",
            "time and created a new sense of hope and opportunity in our\n",
            "country.  Then the jobs plan I presented to Congress, which would\n",
            "create hundreds of thousands of jobs, most of them in the private\n",
            "sector in 1993 and 1994, passed the House of Representatives.  It\n",
            "now has the support of a majority of the United States Senate. \n",
            "But it's been held up by a filibuster of a minority in the\n",
            "Senate, just 43 senators.  They blocked a vote that they know\n",
            "would result in the passage of our bill and the creation of jobs.\n",
            "             \n",
            "             The issue isn't politics; the issue is people. \n",
            "Millions of Americans are waiting for this legislation and\n",
            "counting on it, counting on us in Washington.  But the jobs bill\n",
            "has been grounded by gridlock.  \n",
            "             \n",
            "             I know the American people are tired of business as\n",
            "usual and politics as usual.  I know they don't want us to spin\n",
            "or wheels.  They want the recovery to get moving.  So I have\n",
            "taken a first step to break this gridlock and gone the extra\n",
            "mile.  Yesterday I offered to cut the size of this plan by 25\n",
            "percent -- from $16 billion to $12 billion.  \n",
            "             \n",
            "             It's not what I'd hoped for.  With 16 million\n",
            "Americans looking for full-time work, I simply can't let the bill\n",
            "languish when I know that even a compromise bill will mean\n",
            "hundreds of thousands of jobs for our people.  The mandate is to\n",
            "act to achieve change and move the country forward.  By taking\n",
            "this initiative in the face of an unrelenting Senate talkathon, I\n",
            "think we can respond to your mandate and achieve a significant\n",
            "portion of our original goals.\n",
            "             \n",
            "             First, we want to keep the programs as much as\n",
            "possible that are needed to generate jobs and meet human needs,\n",
            "including highway and road construction, summer jobs for young\n",
            "people, immunization for children, construction of waste water\n",
            "sites, and aid to small businesses.  We also want to keep funding\n",
            "for extended unemployment compensation benefits, for people who\n",
            "have been unemployed for a long time because the economy isn't\n",
            "creating jobs.\n",
            "             \n",
            "             Second, I've recommended that all the other programs\n",
            "in the bill be cut across-the-board by a little more than 40\n",
            "percent.\n",
            "             \n",
            "             And third, I've recommended a new element in this\n",
            "program to help us immediately start our attempt to fight against\n",
            "crime by providing $200 million for cities and towns to rehire\n",
            "police officers who lost their jobs during the recession and put\n",
            "them back to work protecting our people.  I'm also going to fight\n",
            "for a tough crime bill because the people of this country need it\n",
            "and deserve it.\n",
            "             \n",
            "             Now, the people who are filibustering this bill --\n",
            "the Republican senators -- say they won't vote for it because it\n",
            "increases deficit spending, because there's extra spending this\n",
            "year that hasn't already been approved.  That sounds reasonable,\n",
            "doesn't it?  Here's what they don't say.  This program is more\n",
            "than paid for by budget cuts over my five-year budget, and this\n",
            "budget is well within the spending limits already approved by the\n",
            "Congress this year.\n",
            "             \n",
            "             It's amazing to me that many of these same senators\n",
            "who are filibustering the bill voted during the previous\n",
            "administration for billions of dollars of the same kind of\n",
            "emergency spending, and much of it was not designed to put the\n",
            "American people to work.  \n",
            "             \n",
            "             This is not about deficit spending.  We have offered\n",
            "a plan to cut the deficit.  This is about where your priorities\n",
            "are -- on people or on politics.  \n",
            "             \n",
            "             Keep in mind that our jobs bill is paid for dollar\n",
            "for dollar.  It is paid for by budget cuts.  And it's the\n",
            "soundest investment we can now make for ourselves and our\n",
            "children.  I urge all Americans to take another look at this jobs\n",
            "and investment program; to consider again the benefits for all of\n",
            "us when we've helped make more American partners working to\n",
            "ensure the future of our nation and the strength of our economy.\n",
            "             \n",
            "             You know, if every American who wanted a job had\n",
            "one, we wouldn't have a lot of the other problems we have in this\n",
            "country today.  This bill is not a miracle, it's a modest first\n",
            "step to try to set off a job creation explosion in this country\n",
            "again.  But it's a step we ought to take.  And it is fully paid\n",
            "for over the life of our budget.\n",
            "             \n",
            "             Tell your lawmakers what you think.  Tell them how\n",
            "important the bill is.  If it passes, we'll all be winners.\n",
            "             \n",
            "             Good morning, and thank you for listening.\n"
          ]
        }
      ],
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 4811\n",
        "print(newsgroups_train.data[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "outputs": [],
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "eed365b5-6535-44c7-b806-678ffcb93d80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.70930477, 0.67474953, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "985a88c0-80d3-4f03-b30c-d094c8d88587"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4811, 6635, 4253, ..., 9019, 9016, 8748])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "outputs": [],
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "98b91ef2-2d92-43fe-9ad0-6f7f389b77de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_train.target_names[y_train[idx]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "8cd97697-8f9e-4d0c-ed6b-9be9536064ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n"
          ]
        }
      ],
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_train.target_names[y_train[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoNnKwhBqzq"
      },
      "source": [
        "### Modelo de clasificación Naïve Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "TPM0thDaLk0R",
        "outputId": "80e16a4a-55c2-4743-9b5b-464956c59ea9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "outputs": [],
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "2c4b9c29-bb8c-4333-c754-bee202aa7a51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Consigna del desafío 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**Cada experimento realizado debe estar acompañado de una explicación o interpretación de lo observado.**\n",
        "\n",
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**2**. Construir un modelo de clasificación por prototipos (tipo zero-shot). Clasificar los documentos de un conjunto de test comparando cada uno con todos los de entrenamiento y asignar la clase al label del documento del conjunto de entrenamiento con mayor similaridad.\n",
        "\n",
        "**3**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**NO cambiar el hiperparámetro ngram_range de los vectorizadores**.\n",
        "\n",
        "**4**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares.\n",
        "\n",
        "**Elegir las palabras MANUALMENTE para evitar la aparición de términos poco interpretables**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 1\n",
        "\n",
        "#### Observaciones:\n",
        "\n",
        "  * Se observa que en los documentos donde el tema es claro y tiene vocabulario específico (por ejemplo, hardware o motocicletas), el modelo identifica correctamente a sus vecinos más similares dentro de la misma clase.\n",
        "  * En cambio, en textos con lenguaje más general o temáticas superpuestas, las similitudes son bajas y aparecen vecinos de otras categorías.\n",
        "  * Esto indica que TF-IDF funciona bien cuando las palabras clave son distintivas, pero tiene limitaciones para captar el sentido semántico más profundo de los textos."
      ],
      "metadata": {
        "id": "Y3Li8PYKy461"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "textos = newsgroups_train.data\n",
        "etiquetas = newsgroups_train.target\n",
        "nombres_clases = newsgroups_train.target_names\n",
        "\n",
        "print(\"Cantidad de documentos:\", len(textos))\n",
        "print(\"Ejemplo de texto:\\n\")\n",
        "print(textos[0][:500])\n",
        "print()\n",
        "\n",
        "# Elegir el tipo de vectorizador\n",
        "vectorizador = TfidfVectorizer()\n",
        "\n",
        "\n",
        "X = vectorizador.fit_transform(textos)\n",
        "\n",
        "print(\"Forma de la matriz:\", X.shape)\n",
        "print(\"Cada fila representa un documento y cada columna una palabra distinta.\")\n",
        "print()\n",
        "\n",
        "# Elegir 5 documentos especificos\n",
        "indices_aleatorios = [200, 450, 3000, 3001, 10]\n",
        "\n",
        "print(\"indices seleccionados:\", indices_aleatorios)\n",
        "print()\n",
        "\n",
        "\n",
        "# Calcular la similitud del coseno entre documentos\n",
        "for idx in indices_aleatorios:\n",
        "    print(\"================================\" )\n",
        "    print(f\"Documento base #{idx}\")\n",
        "    print(\"Clase:\", nombres_clases[etiquetas[idx]])\n",
        "    print(\"Texto (primeras 500 letras):\")\n",
        "    print(textos[idx][:500].replace('\\n', ' ') + \"...\")\n",
        "    print()\n",
        "\n",
        "    # Calcular similitud del documento elegido con todos los demás\n",
        "    similitudes = cosine_similarity(X[idx], X)[0]\n",
        "\n",
        "    # Obtener los 5 indices con mayor similitud (excepto el mismo documento)\n",
        "    indices_mas_similares = np.argsort(similitudes)[::-1][1:6]\n",
        "\n",
        "    print(\"Documentos mas similares:\")\n",
        "    for i, idx_sim in enumerate(indices_mas_similares, start=1):\n",
        "        print(f\"{i}) Documento #{idx_sim:>6} | Similitud: {similitudes[idx_sim]:.3f} | \"\n",
        "              f\"Clase: {nombres_clases[etiquetas[idx_sim]]}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejmasK8ey7f8",
        "outputId": "868b9dfb-e558-471c-9dad-b8b2c3e9784d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 11314\n",
            "Ejemplo de texto:\n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Forma de la matriz: (11314, 101631)\n",
            "Cada fila representa un documento y cada columna una palabra distinta.\n",
            "\n",
            "indices seleccionados: [200, 450, 3000, 3001, 10]\n",
            "\n",
            "================================\n",
            "Documento base #200\n",
            "Clase: talk.politics.guns\n",
            "Texto (primeras 500 letras):\n",
            " I first read and consulted rec.guns in the summer of 1991.  I just purchased my first firearm in early March of this year....\n",
            "\n",
            "Documentos mas similares:\n",
            "1) Documento #  6538 | Similitud: 0.312 | Clase: talk.politics.guns\n",
            "2) Documento #  4823 | Similitud: 0.191 | Clase: rec.sport.baseball\n",
            "3) Documento #  1214 | Similitud: 0.180 | Clase: rec.autos\n",
            "4) Documento #   532 | Similitud: 0.180 | Clase: talk.politics.guns\n",
            "5) Documento #  2519 | Similitud: 0.173 | Clase: rec.motorcycles\n",
            "\n",
            "================================\n",
            "Documento base #450\n",
            "Clase: comp.sys.ibm.pc.hardware\n",
            "Texto (primeras 500 letras):\n",
            "      In addition to startup time, I leave things running because my PC doubles as  a fax machine.   However, this is off the original subject. I didn't get the replies on BIOS,  CMOS, and DOS clock/date logic. All I know is that I've been running this way  for many months and it is only recently, the last month, that I have noticed  the intermittent clock problem. As I stated, it is not always the date that  doesn't roll forward, sometimes I notice that the clock is several minutes  behind wher...\n",
            "\n",
            "Documentos mas similares:\n",
            "1) Documento #  2075 | Similitud: 0.431 | Clase: comp.sys.ibm.pc.hardware\n",
            "2) Documento #   943 | Similitud: 0.306 | Clase: comp.sys.ibm.pc.hardware\n",
            "3) Documento #  1339 | Similitud: 0.282 | Clase: comp.sys.ibm.pc.hardware\n",
            "4) Documento #  7591 | Similitud: 0.272 | Clase: comp.sys.ibm.pc.hardware\n",
            "5) Documento #   685 | Similitud: 0.251 | Clase: comp.sys.ibm.pc.hardware\n",
            "\n",
            "================================\n",
            "Documento base #3000\n",
            "Clase: talk.politics.mideast\n",
            "Texto (primeras 500 letras):\n",
            "  That's why the Zionists decided that Zion must be Gentile-rein. What?!  They didn't?!  You mean to tell me that the early Zionists actually granted CITIZENSHIP in the Jewish state to Christian and Muslim people, too?    It seems, Elias, that your \"first point to note\" is wrong, so the rest of your posting isn't worth much, either.  Ta ta... ...\n",
            "\n",
            "Documentos mas similares:\n",
            "1) Documento #  8097 | Similitud: 0.224 | Clase: comp.sys.ibm.pc.hardware\n",
            "2) Documento # 10836 | Similitud: 0.201 | Clase: alt.atheism\n",
            "3) Documento #  8726 | Similitud: 0.198 | Clase: talk.politics.mideast\n",
            "4) Documento #  8754 | Similitud: 0.195 | Clase: talk.religion.misc\n",
            "5) Documento # 10229 | Similitud: 0.194 | Clase: talk.religion.misc\n",
            "\n",
            "================================\n",
            "Documento base #3001\n",
            "Clase: rec.sport.baseball\n",
            "Texto (primeras 500 letras):\n",
            "      If I remember correctly (Which is always in doubt), Horner's signing with the Braves was contingent on starting in Atlanta.  I think he could have gone back to Arizona St. for one more year if he hadn't signed.  Anyhow, the Braves did try to send him to Richmond once; it lead to a week-long walkout.  Methinks Horner had no work ethic before he was drafted, and minor league play wouldn't have helped. But his raw talent would have gotten him into the ML, and it did keep him there for a while...\n",
            "\n",
            "Documentos mas similares:\n",
            "1) Documento #  3576 | Similitud: 0.279 | Clase: rec.sport.hockey\n",
            "2) Documento #  1788 | Similitud: 0.279 | Clase: rec.sport.baseball\n",
            "3) Documento # 11285 | Similitud: 0.246 | Clase: rec.sport.baseball\n",
            "4) Documento #  9670 | Similitud: 0.245 | Clase: soc.religion.christian\n",
            "5) Documento #  4401 | Similitud: 0.239 | Clase: misc.forsale\n",
            "\n",
            "================================\n",
            "Documento base #10\n",
            "Clase: rec.motorcycles\n",
            "Texto (primeras 500 letras):\n",
            "I have a line on a Ducati 900GTS 1978 model with 17k on the clock.  Runs very well, paint is the bronze/brown/orange faded out, leaks a bit of oil and pops out of 1st with hard accel.  The shop will fix trans and oil  leak.  They sold the bike to the 1 and only owner.  They want $3495, and I am thinking more like $3K.  Any opinions out there?  Please email me. Thanks.  It would be a nice stable mate to the Beemer.  Then I'll get a jap bike and call myself Axis Motors!  --  ----------------------...\n",
            "\n",
            "Documentos mas similares:\n",
            "1) Documento #  3543 | Similitud: 0.496 | Clase: rec.motorcycles\n",
            "2) Documento #  9171 | Similitud: 0.459 | Clase: rec.motorcycles\n",
            "3) Documento #  4930 | Similitud: 0.169 | Clase: rec.motorcycles\n",
            "4) Documento #  5063 | Similitud: 0.165 | Clase: rec.motorcycles\n",
            "5) Documento #  4211 | Similitud: 0.160 | Clase: rec.motorcycles\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 2\n",
        "\n",
        "#### Observaciones:\n",
        "  * El modelo de clasificación por prototipos predice cada documento del conjunto de test según la mayor similitud de coseno con los documentos de entrenamiento.\n",
        "  * A diferencia de otros modelos, no aprende patrones, sino que asigna la clase del texto más parecido.\n",
        "  * El proceso resulta computacionalmente costoso, ya que compara cada documento de test con todos los de train.\n",
        "  * Los resultados son coherentes en categorías bien definidas, aunque el rendimiento general es bajo, reflejando las limitaciones de este método tan directo."
      ],
      "metadata": {
        "id": "jG95LTgF4ASZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Cargar datos de entrenamiento y test\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test  = fetch_20newsgroups(subset='test',  remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train_text = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "X_test_text  = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "nombres_clases = newsgroups_train.target_names\n",
        "\n",
        "\n",
        "#  Vectorizar con TF-IDF\n",
        "vectorizador = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizador.fit_transform(X_train_text)\n",
        "X_test  = vectorizador.transform(X_test_text)\n",
        "\n",
        "\n",
        "# Clasificador por prototipos . Para cada test, buscar el train más similar\n",
        "y_pred = []\n",
        "\n",
        "print(\"Calculando similitudes entre TODOS los documentos de test y de train...\")\n",
        "sim = cosine_similarity(X_test, X_train)\n",
        "\n",
        "# Para cada documento de test, buscamos el índice del train más parecido\n",
        "idx_max = np.argmax(sim, axis=1)\n",
        "\n",
        "# Obtenemos la clase correspondiente del train\n",
        "y_pred = y_train[idx_max]\n",
        "\n",
        "\n",
        "#  Evaluar el rendimiento\n",
        "print(\"=== Resultados Clasificador por Prototipos ===\")\n",
        "print(\"F1-macro:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=nombres_clases, digits=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTAEOCx04CGD",
        "outputId": "58c90e98-9860-4f1b-ebea-bf7cd1776148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculando similitudes entre TODOS los documentos de test y de train...\n",
            "\n",
            "=== Resultados Clasificador por Prototipos ===\n",
            "F1-macro: 0.5049911553681621\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism      0.366     0.508     0.425       319\n",
            "           comp.graphics      0.543     0.483     0.512       389\n",
            " comp.os.ms-windows.misc      0.506     0.457     0.480       394\n",
            "comp.sys.ibm.pc.hardware      0.515     0.520     0.518       392\n",
            "   comp.sys.mac.hardware      0.535     0.499     0.516       385\n",
            "          comp.windows.x      0.701     0.592     0.642       395\n",
            "            misc.forsale      0.629     0.462     0.533       390\n",
            "               rec.autos      0.406     0.576     0.476       396\n",
            "         rec.motorcycles      0.635     0.515     0.569       398\n",
            "      rec.sport.baseball      0.645     0.537     0.586       397\n",
            "        rec.sport.hockey      0.748     0.722     0.735       399\n",
            "               sci.crypt      0.552     0.588     0.570       396\n",
            "         sci.electronics      0.531     0.328     0.406       393\n",
            "                 sci.med      0.652     0.492     0.561       396\n",
            "               sci.space      0.644     0.505     0.566       394\n",
            "  soc.religion.christian      0.451     0.580     0.508       398\n",
            "      talk.politics.guns      0.454     0.470     0.462       364\n",
            "   talk.politics.mideast      0.363     0.604     0.453       376\n",
            "      talk.politics.misc      0.293     0.323     0.307       310\n",
            "      talk.religion.misc      0.261     0.295     0.277       251\n",
            "\n",
            "                accuracy                          0.509      7532\n",
            "               macro avg      0.521     0.503     0.505      7532\n",
            "            weighted avg      0.531     0.509     0.513      7532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 3\n",
        "\n",
        "#### Observaciones:\n",
        "  * El modelo Naïve Bayes logra un desempeño alto comparado con el clasificador por prototipos.\n",
        "  * Se entrenaron variantes Multinomial y Complement ajustando parámetros del vectorizador y del modelo para maximizar el F1-macro.\n",
        "  * Ambos alcanzan valores cercanos a 0.7 - 0.8, siendo ComplementNB ligeramente superior.\n",
        "  * TF-IDF combinado con Naïve Bayes demuestra buena capacidad para separar categorías basadas en el vocabulario distintivo de cada tema."
      ],
      "metadata": {
        "id": "7Eqqp4e-7yd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Cargar los datos\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test  = fetch_20newsgroups(subset='test',  remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_train_text = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "X_test_text  = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "nombres_clases = newsgroups_train.target_names\n",
        "\n",
        "\n",
        "#  Definir el pipeline (vectorizador + clasificador)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "\n",
        "# Definir la grilla de hiperparámetros a probar\n",
        "param_grid = {\n",
        "    'tfidf__min_df': [1, 2],\n",
        "    'tfidf__max_df': [0.9],\n",
        "    'tfidf__sublinear_tf': [True, False],\n",
        "    'tfidf__norm': ['l2', None],\n",
        "    'clf__alpha': [0.5, 0.2, 0.1]\n",
        "}\n",
        "\n",
        "\n",
        "# Buscar la mejor combinación\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=2,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Entrenando modelo MultinomialNB...\")\n",
        "grid.fit(X_train_text, y_train)\n",
        "\n",
        "print(\"\\nMejor combinación de parámetros:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Mejor F1-macro (validación cruzada):\", grid.best_score_)\n",
        "\n",
        "\n",
        "# Evaluar en el conjunto de test\n",
        "y_pred = grid.best_estimator_.predict(X_test_text)\n",
        "print(\"\\n=== Resultados MultinomialNB en Test ===\")\n",
        "print(\"F1-macro:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(classification_report(y_test, y_pred, target_names=nombres_clases, digits=3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ahora con ComplementNB\n",
        "pipeline_cnb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', ComplementNB())\n",
        "])\n",
        "\n",
        "param_grid_cnb = {\n",
        "    'tfidf__min_df': [1, 2],\n",
        "    'tfidf__max_df': [0.9],\n",
        "    'tfidf__sublinear_tf': [True, False],\n",
        "    'tfidf__norm': ['l2', None],\n",
        "    'clf__alpha': [0.5, 0.2, 0.1]\n",
        "}\n",
        "\n",
        "grid_cnb = GridSearchCV(\n",
        "    pipeline_cnb,\n",
        "    param_grid=param_grid_cnb,\n",
        "    scoring='f1_macro',\n",
        "    cv=2,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nEntrenando modelo ComplementNB...\")\n",
        "grid_cnb.fit(X_train_text, y_train)\n",
        "\n",
        "print(\"\\nMejor combinación de parámetros (ComplementNB):\")\n",
        "print(grid_cnb.best_params_)\n",
        "print(\"Mejor F1-macro (validación cruzada):\", grid_cnb.best_score_)\n",
        "\n",
        "y_pred_cnb = grid_cnb.best_estimator_.predict(X_test_text)\n",
        "print(\"\\n=== Resultados ComplementNB en Test ===\")\n",
        "print(\"F1-macro:\", f1_score(y_test, y_pred_cnb, average='macro'))\n",
        "print(classification_report(y_test, y_pred_cnb, target_names=nombres_clases, digits=3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukemaxjp7yK2",
        "outputId": "2882b5d8-dd59-4bcb-8be2-a69e4d493870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando modelo MultinomialNB...\n",
            "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
            "\n",
            "Mejor combinación de parámetros:\n",
            "{'clf__alpha': 0.1, 'tfidf__max_df': 0.9, 'tfidf__min_df': 2, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': False}\n",
            "Mejor F1-macro (validación cruzada): 0.7065375081505362\n",
            "\n",
            "=== Resultados MultinomialNB en Test ===\n",
            "F1-macro: 0.6726552851196274\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism      0.650     0.367     0.469       319\n",
            "           comp.graphics      0.645     0.715     0.678       389\n",
            " comp.os.ms-windows.misc      0.678     0.551     0.608       394\n",
            "comp.sys.ibm.pc.hardware      0.614     0.722     0.664       392\n",
            "   comp.sys.mac.hardware      0.747     0.696     0.720       385\n",
            "          comp.windows.x      0.826     0.747     0.785       395\n",
            "            misc.forsale      0.841     0.744     0.789       390\n",
            "               rec.autos      0.787     0.747     0.767       396\n",
            "         rec.motorcycles      0.803     0.759     0.780       398\n",
            "      rec.sport.baseball      0.921     0.819     0.867       397\n",
            "        rec.sport.hockey      0.602     0.925     0.729       399\n",
            "               sci.crypt      0.703     0.773     0.736       396\n",
            "         sci.electronics      0.717     0.567     0.634       393\n",
            "                 sci.med      0.846     0.778     0.811       396\n",
            "               sci.space      0.775     0.779     0.777       394\n",
            "  soc.religion.christian      0.442     0.907     0.594       398\n",
            "      talk.politics.guns      0.560     0.747     0.640       364\n",
            "   talk.politics.mideast      0.825     0.787     0.805       376\n",
            "      talk.politics.misc      0.724     0.397     0.512       310\n",
            "      talk.religion.misc      0.522     0.048     0.088       251\n",
            "\n",
            "                accuracy                          0.697      7532\n",
            "               macro avg      0.711     0.679     0.673      7532\n",
            "            weighted avg      0.716     0.697     0.687      7532\n",
            "\n",
            "\n",
            "Entrenando modelo ComplementNB...\n",
            "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
            "\n",
            "Mejor combinación de parámetros (ComplementNB):\n",
            "{'clf__alpha': 0.2, 'tfidf__max_df': 0.9, 'tfidf__min_df': 1, 'tfidf__norm': 'l2', 'tfidf__sublinear_tf': True}\n",
            "Mejor F1-macro (validación cruzada): 0.7458932598622934\n",
            "\n",
            "=== Resultados ComplementNB en Test ===\n",
            "F1-macro: 0.6994500640906747\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism      0.323     0.442     0.373       319\n",
            "           comp.graphics      0.721     0.717     0.719       389\n",
            " comp.os.ms-windows.misc      0.740     0.543     0.627       394\n",
            "comp.sys.ibm.pc.hardware      0.634     0.704     0.667       392\n",
            "   comp.sys.mac.hardware      0.783     0.722     0.751       385\n",
            "          comp.windows.x      0.794     0.803     0.798       395\n",
            "            misc.forsale      0.755     0.736     0.745       390\n",
            "               rec.autos      0.814     0.750     0.781       396\n",
            "         rec.motorcycles      0.815     0.784     0.799       398\n",
            "      rec.sport.baseball      0.916     0.854     0.884       397\n",
            "        rec.sport.hockey      0.869     0.947     0.906       399\n",
            "               sci.crypt      0.754     0.811     0.781       396\n",
            "         sci.electronics      0.716     0.565     0.632       393\n",
            "                 sci.med      0.780     0.806     0.793       396\n",
            "               sci.space      0.794     0.812     0.803       394\n",
            "  soc.religion.christian      0.562     0.894     0.690       398\n",
            "      talk.politics.guns      0.603     0.717     0.655       364\n",
            "   talk.politics.mideast      0.774     0.848     0.810       376\n",
            "      talk.politics.misc      0.684     0.426     0.525       310\n",
            "      talk.religion.misc      0.532     0.163     0.250       251\n",
            "\n",
            "                accuracy                          0.718      7532\n",
            "               macro avg      0.718     0.702     0.699      7532\n",
            "            weighted avg      0.726     0.718     0.713      7532\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Punto 4\n",
        "\n",
        "#### Observaciones:\n",
        "\n",
        "  * El análisis de similitud entre palabras muestra que TF-IDF también puede representar relaciones semánticas básicas.\n",
        "  * Palabras que suelen aparecer en los mismos contextos presentan alta similitud de coseno, reflejando asociaciones temáticas (por ejemplo, space-nasa, car-engine).\n",
        "  * Sin embargo, el método no capta significados profundos ni sinónimos, ya que se basa solo en la coocurrencia dentro de los documentos."
      ],
      "metadata": {
        "id": "maLjqPqLB7Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "textos = newsgroups_train.data\n",
        "etiquetas = newsgroups_train.target\n",
        "nombres_clases = newsgroups_train.target_names\n",
        "\n",
        "vectorizador = TfidfVectorizer()\n",
        "X = vectorizador.fit_transform(textos)\n",
        "print(\"Matriz documento palabra:\", X.shape)\n",
        "\n",
        "\n",
        "# Transponer la matriz para obtener documento palabra\n",
        "X_td = X.T     # ahora las filas son palabras, las columnas documentos\n",
        "print(\"Matriz documento palabra:\", X_td.shape)\n",
        "\n",
        "# Crear diccionarios para acceder por nombre\n",
        "vocabulario = vectorizador.vocabulary_             # palabra -> índice\n",
        "idx_to_word = {v: k for k, v in vocabulario.items()}  # índice -> palabra\n",
        "\n",
        "\n",
        "# Elegir palabras manualmente\n",
        "palabras = [\"space\", \"god\", \"car\", \"windows\", \"hockey\"]\n",
        "\n",
        "# Función para buscar palabras similares\n",
        "def palabras_mas_similares(palabra, X_td, vocabulario, idx_to_word, k=5):\n",
        "\n",
        "    idx = vocabulario[palabra]\n",
        "\n",
        "    # Similaridad de coseno entre esta palabra y todas las demás\n",
        "    similitudes = cosine_similarity(X_td[idx], X_td)[0]\n",
        "\n",
        "    # Ordenar de mayor a menor y descartar la misma palabra\n",
        "    indices_ordenados = np.argsort(similitudes)[::-1]\n",
        "    indices_ordenados = [i for i in indices_ordenados if i != idx][:k]\n",
        "\n",
        "    resultados = [(idx_to_word[i], similitudes[i]) for i in indices_ordenados]\n",
        "    return resultados\n",
        "\n",
        "# Mostrar los resultados\n",
        "for palabra in palabras:\n",
        "    print(\"===================\")\n",
        "    print(f\"Palabra base: {palabra}\")\n",
        "    similares = palabras_mas_similares(palabra, X_td, vocabulario, idx_to_word, k=5)\n",
        "    for sim_word, score in similares:\n",
        "        print(f\"  {sim_word:<20}  similitud: {score:.3f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jG53TLXCATd",
        "outputId": "1b3e861b-49de-4ef0-9769-6f472516eb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz documento palabra: (11314, 101631)\n",
            "Matriz documento palabra: (101631, 11314)\n",
            "===================\n",
            "Palabra base: space\n",
            "  nasa                  similitud: 0.330\n",
            "  seds                  similitud: 0.297\n",
            "  shuttle               similitud: 0.293\n",
            "  enfant                similitud: 0.280\n",
            "  seti                  similitud: 0.246\n",
            "\n",
            "===================\n",
            "Palabra base: god\n",
            "  jesus                 similitud: 0.269\n",
            "  bible                 similitud: 0.262\n",
            "  that                  similitud: 0.256\n",
            "  existence             similitud: 0.255\n",
            "  christ                similitud: 0.251\n",
            "\n",
            "===================\n",
            "Palabra base: car\n",
            "  cars                  similitud: 0.180\n",
            "  criterium             similitud: 0.177\n",
            "  civic                 similitud: 0.175\n",
            "  owner                 similitud: 0.169\n",
            "  dealer                similitud: 0.168\n",
            "\n",
            "===================\n",
            "Palabra base: windows\n",
            "  dos                   similitud: 0.304\n",
            "  ms                    similitud: 0.232\n",
            "  microsoft             similitud: 0.222\n",
            "  nt                    similitud: 0.214\n",
            "  for                   similitud: 0.193\n",
            "\n",
            "===================\n",
            "Palabra base: hockey\n",
            "  ncaa                  similitud: 0.274\n",
            "  nhl                   similitud: 0.265\n",
            "  affiliates            similitud: 0.248\n",
            "  xenophobes            similitud: 0.243\n",
            "  sportschannel         similitud: 0.223\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}